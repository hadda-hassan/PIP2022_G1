{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"class_weight\", \"max_depth\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[16:01:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.995767 using {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993122 (0.013757) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995767 (0.008466) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995767 (0.008466) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.994709 (0.010582) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "60 fits failed out of a total of 315.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\pipelinehelper\\__init__.py\", line 159, in fit\n",
      "    return self.selected_model.fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\pipelinehelper\\__init__.py\", line 159, in fit\n",
      "    return self.selected_model.fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\pipelinehelper\\__init__.py\", line 159, in fit\n",
      "    return self.selected_model.fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\pipelinehelper\\__init__.py\", line 159, in fit\n",
      "    return self.selected_model.fit(x, y, **kwargs)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\MSI GT 70 APACHE PRO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.99153439 0.99259259        nan 0.99259259 0.99365079 0.99259259\n",
      "        nan        nan        nan 0.99259259 0.99153439        nan\n",
      " 0.99259259 0.99259259 0.99259259        nan        nan        nan\n",
      " 0.99259259 0.99153439        nan 0.99365079 0.99365079 0.99365079\n",
      "        nan        nan        nan 0.9952381  0.9952381  0.9952381\n",
      " 0.9952381  0.9952381  0.9952381  0.9952381  0.9952381  0.9952381\n",
      " 0.9952381  0.9952381  0.9952381  0.9952381  0.9952381  0.99312169\n",
      " 0.9952381  0.9952381  0.9952381  0.99365079 0.9957672  0.99047619\n",
      " 0.99365079 0.9957672  0.99047619 0.99365079 0.99470899 0.99047619\n",
      " 0.9952381  0.9952381  0.9952381  0.9952381  0.9952381  0.9952381\n",
      " 0.99153439 0.99153439 0.99153439]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!pip install pipelinehelper\n",
    "#!pip install xgboost\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from pipelinehelper import PipelineHelper\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "#SVM\n",
    "#RANDOM FOREST\n",
    "#XGBOOST\n",
    "#LOGISTIC REGRESSION\n",
    "#RNN\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "y_iris = (y_iris>0).astype(int)\n",
    "y_iris[0] = 1\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler())\n",
    "    ])),\n",
    "\n",
    "    ('classifier', PipelineHelper([\n",
    "         ('lg',LogisticRegression()),\n",
    "        ('rf',RandomForestClassifier()),\n",
    "        ('svc',SVC()),\n",
    "        ('xgb',XGBClassifier())\n",
    "        \n",
    "       \n",
    "        \n",
    "    ])),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "        'std__with_mean': [True,]\n",
    "#         'std__with_std': [True, False],\n",
    "#         'max__copy': [True],  # just for displaying\n",
    "    }),\n",
    "    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "        'lg__class_weight' :['balanced' ], \n",
    "         'lg__penalty' :['l1', 'l2', 'elasticnet'],\n",
    "         'lg__solver' : ['liblinear', 'saga','lbfgs'],\n",
    "          'lg__C' : [ 100 ,10, 1.0],\n",
    "        'rf__class_weight' :['balanced'], \n",
    "        'rf__n_estimators' : [10,100,300],\n",
    "        'rf__max_depth' : [10,20,30],\n",
    "        'rf__max_features' : ['auto',None],\n",
    "        'svc__C': [100,10,1],\n",
    "        'svc__kernel' : ['linear', 'poly', 'rbf'],\n",
    "        'svc__class_weight' :['balanced'], \n",
    "        \n",
    "        'xgb__booster': [\"gbtree\",'dart','gblinear'],\n",
    "        'xgb__max_depth' : [6 ,10, 20],\n",
    "        'xgb__class_weight' :['balanced'], \n",
    "        \n",
    "   })\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, scoring='roc_auc', verbose=1)\n",
    "grid.fit(X_iris, y_iris)\n",
    "\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.995767 using {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 100, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 10, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.992593 (0.014815) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l1', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'liblinear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'saga'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "nan (nan) with: {'classifier__selected_model': ('lg', {'C': 1.0, 'class_weight': 'balanced', 'penalty': 'elasticnet', 'solver': 'lbfgs'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 10, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 20, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993122 (0.013757) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': 'auto', 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 100}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('rf', {'class_weight': 'balanced', 'max_depth': 30, 'max_features': None, 'n_estimators': 300}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995767 (0.008466) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 100, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995767 (0.008466) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 10, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.993651 (0.012698) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'linear'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.994709 (0.010582) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'poly'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.990476 (0.019048) with: {'classifier__selected_model': ('svc', {'C': 1, 'class_weight': 'balanced', 'kernel': 'rbf'}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'gbtree', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.995238 (0.009524) with: {'classifier__selected_model': ('xgb', {'booster': 'dart', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 6}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 10}), 'scaler__selected_model': ('std', {'with_mean': True})}\n",
      "0.991534 (0.016931) with: {'classifier__selected_model': ('xgb', {'booster': 'gblinear', 'class_weight': 'balanced', 'max_depth': 20}), 'scaler__selected_model': ('std', {'with_mean': True})}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>modele</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>{'classifier__selected_model': ('svc', {'C': 1...</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.995767</td>\n",
       "      <td>svc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>{'classifier__selected_model': ('rf', {'class_...</td>\n",
       "      <td>0.011520</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>rf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>{'classifier__selected_model': ('xgb', {'boost...</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.995238</td>\n",
       "      <td>xgb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{'classifier__selected_model': ('lg', {'C': 1....</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.993651</td>\n",
       "      <td>lg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_score_time  std_score_time  mean_test_score modele\n",
       "49  {'classifier__selected_model': ('svc', {'C': 1...         0.001137        0.000287         0.995767    svc\n",
       "37  {'classifier__selected_model': ('rf', {'class_...         0.011520        0.002282         0.995238     rf\n",
       "56  {'classifier__selected_model': ('xgb', {'boost...         0.006996        0.000633         0.995238    xgb\n",
       "23  {'classifier__selected_model': ('lg', {'C': 1....         0.001399        0.000490         0.993651     lg"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cv_results = pd.DataFrame.from_dict(grid.cv_results_)\n",
    "df = cv_results[['params', 'mean_score_time', 'std_score_time', 'mean_test_score']]  #RAJOUTER LES AUTRES SCORES POSSIBLES !!\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "d = df.dropna().sort_values('mean_test_score',ascending = False)\n",
    "d['modele'] = [ i['classifier__selected_model'][0] for i in d['params']]\n",
    "d.drop_duplicates('modele',keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trouver pour chaque modele la meilleure combinaison de paramtres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_classifier__selected_model', 'param_scaler__selected_model', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_classifier__selected_model', 'param_scaler__selected_model', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(grid.cv_results_.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_classifier__selected_model', 'param_scaler__selected_model', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb', 'xgb', 'xgb', 'xgb', 'xgb', 'xgb', 'xgb', 'xgb', 'xgb']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ i['classifier__selected_model'][0] for i in grid.cv_results_['params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler',\n",
       "                 PipelineHelper(available_models={'std': StandardScaler()},\n",
       "                                selected_model=StandardScaler())),\n",
       "                ('classifier',\n",
       "                 PipelineHelper(available_models={'xgb': XGBClassifier(base_score=None,\n",
       "                                                                       booster='gbtree',\n",
       "                                                                       class_weight='balanced',\n",
       "                                                                       colsample_bylevel=None,\n",
       "                                                                       colsample_bynode=None,\n",
       "                                                                       colsample_bytree=None,\n",
       "                                                                       enable_categorical=False,\n",
       "                                                                       gamma=None,\n",
       "                                                                       gpu_id...\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints='',\n",
       "                                                             learning_rate=0.300000012,\n",
       "                                                             max_delta_step=0,\n",
       "                                                             max_depth=6,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints='()',\n",
       "                                                             n_estimators=100,\n",
       "                                                             n_jobs=8,\n",
       "                                                             num_parallel_tree=1,\n",
       "                                                             predictor='auto',\n",
       "                                                             random_state=0,\n",
       "                                                             reg_alpha=0,\n",
       "                                                             reg_lambda=1,\n",
       "                                                             scale_pos_weight=1,\n",
       "                                                             subsample=1,\n",
       "                                                             tree_method='exact',\n",
       "                                                             validate_parameters=1,\n",
       "                                                             verbosity=None)))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0441844 , 0.04497409, 0.04377456, 0.2158762 , 0.21507473,\n",
       "       0.19909616, 0.02500677, 0.02218742, 0.0221868 ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_['mean_fit_time']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
